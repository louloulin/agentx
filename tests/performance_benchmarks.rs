//! AgentXæ€§èƒ½åŸºå‡†æµ‹è¯•
//! 
//! éªŒè¯ç³»ç»Ÿæ€§èƒ½æ˜¯å¦è¾¾åˆ°è®¾è®¡ç›®æ ‡ï¼š
//! - gRPCå»¶è¿Ÿ: < 5ms
//! - æ¶ˆæ¯è·¯ç”±å»¶è¿Ÿ: < 10ms  
//! - ååé‡: æ”¯æŒ10,000+ å¹¶å‘Agent
//! - æ’ä»¶å¯åŠ¨æ—¶é—´: < 3ç§’
//! - æ’ä»¶æ•…éšœæ¢å¤: < 1ç§’

use std::time::{Duration, Instant};
use std::sync::Arc;
use tokio::sync::{RwLock, Semaphore};
use agentx_a2a::{
    A2AProtocolEngine, ProtocolEngineConfig, AgentInfo, AgentStatus,
    A2AMessage, MessageRole,
};
use agentx_cluster::{ClusterManager, ClusterConfig};

/// æ€§èƒ½åŸºå‡†æµ‹è¯•é…ç½®
#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    /// å¹¶å‘Agentæ•°é‡
    pub concurrent_agents: usize,
    /// æ¶ˆæ¯æ•°é‡
    pub message_count: usize,
    /// æµ‹è¯•æŒç»­æ—¶é—´
    pub duration: Duration,
    /// é¢„çƒ­æ—¶é—´
    pub warmup_duration: Duration,
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            concurrent_agents: 1000,
            message_count: 10000,
            duration: Duration::from_secs(60),
            warmup_duration: Duration::from_secs(10),
        }
    }
}

/// æ€§èƒ½æµ‹è¯•ç»“æœ
#[derive(Debug, Clone)]
pub struct BenchmarkResults {
    /// å¹³å‡å»¶è¿Ÿ (æ¯«ç§’)
    pub avg_latency_ms: f64,
    /// P95å»¶è¿Ÿ (æ¯«ç§’)
    pub p95_latency_ms: f64,
    /// P99å»¶è¿Ÿ (æ¯«ç§’)
    pub p99_latency_ms: f64,
    /// ååé‡ (æ“ä½œ/ç§’)
    pub throughput_ops_per_sec: f64,
    /// é”™è¯¯ç‡ (%)
    pub error_rate_percent: f64,
    /// å†…å­˜ä½¿ç”¨ (MB)
    pub memory_usage_mb: f64,
    /// CPUä½¿ç”¨ç‡ (%)
    pub cpu_usage_percent: f64,
}

/// æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
pub struct PerformanceBenchmarks {
    config: BenchmarkConfig,
    a2a_engine: Arc<RwLock<A2AProtocolEngine>>,
    cluster_manager: Arc<ClusterManager>,
}

impl PerformanceBenchmarks {
    /// åˆ›å»ºæ–°çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶
    pub async fn new(config: BenchmarkConfig) -> Result<Self, Box<dyn std::error::Error>> {
        // åˆå§‹åŒ–A2Aåè®®å¼•æ“
        let engine_config = ProtocolEngineConfig::default();
        let a2a_engine = Arc::new(RwLock::new(A2AProtocolEngine::new(engine_config)));

        // åˆå§‹åŒ–é›†ç¾¤ç®¡ç†å™¨
        let cluster_config = ClusterConfig::default();
        let cluster_manager = Arc::new(ClusterManager::new(cluster_config).await?);

        Ok(Self {
            config,
            a2a_engine,
            cluster_manager,
        })
    }

    /// è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•
    pub async fn run_full_benchmark(&self) -> Result<Vec<BenchmarkResults>, Box<dyn std::error::Error>> {
        println!("ğŸš€ å¼€å§‹AgentXæ€§èƒ½åŸºå‡†æµ‹è¯•");
        println!("ğŸ“Š æµ‹è¯•é…ç½®: {:?}", self.config);

        let mut results = Vec::new();

        // 1. A2Aåè®®æ€§èƒ½æµ‹è¯•
        println!("\nğŸ“¡ æµ‹è¯•A2Aåè®®æ€§èƒ½...");
        let a2a_results = self.benchmark_a2a_protocol().await?;
        results.push(a2a_results);

        // 2. æ¶ˆæ¯è·¯ç”±æ€§èƒ½æµ‹è¯•
        println!("\nğŸ”€ æµ‹è¯•æ¶ˆæ¯è·¯ç”±æ€§èƒ½...");
        let routing_results = self.benchmark_message_routing().await?;
        results.push(routing_results);

        // 3. Agentæ³¨å†Œå’Œå‘ç°æ€§èƒ½æµ‹è¯•
        println!("\nğŸ‘¥ æµ‹è¯•Agentæ³¨å†Œå’Œå‘ç°æ€§èƒ½...");
        let registry_results = self.benchmark_agent_registry().await?;
        results.push(registry_results);

        // 4. é›†ç¾¤ç®¡ç†æ€§èƒ½æµ‹è¯•
        println!("\nğŸ—ï¸ æµ‹è¯•é›†ç¾¤ç®¡ç†æ€§èƒ½...");
        let cluster_results = self.benchmark_cluster_management().await?;
        results.push(cluster_results);

        // 5. å¹¶å‘è´Ÿè½½æµ‹è¯•
        println!("\nâš¡ æµ‹è¯•å¹¶å‘è´Ÿè½½æ€§èƒ½...");
        let concurrent_results = self.benchmark_concurrent_load().await?;
        results.push(concurrent_results);

        // 6. å†…å­˜å’Œèµ„æºä½¿ç”¨æµ‹è¯•
        println!("\nğŸ’¾ æµ‹è¯•èµ„æºä½¿ç”¨æƒ…å†µ...");
        let resource_results = self.benchmark_resource_usage().await?;
        results.push(resource_results);

        println!("\nâœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ");
        self.print_summary(&results);

        Ok(results)
    }

    /// A2Aåè®®æ€§èƒ½åŸºå‡†æµ‹è¯•
    async fn benchmark_a2a_protocol(&self) -> Result<BenchmarkResults, Box<dyn std::error::Error>> {
        let mut latencies = Vec::new();
        let mut errors = 0;
        let start_time = Instant::now();

        // é¢„çƒ­
        for _ in 0..100 {
            let _ = self.send_test_message().await;
        }

        // å®é™…æµ‹è¯•
        for i in 0..self.config.message_count {
            let msg_start = Instant::now();
            
            match self.send_test_message().await {
                Ok(_) => {
                    let latency = msg_start.elapsed();
                    latencies.push(latency.as_secs_f64() * 1000.0); // è½¬æ¢ä¸ºæ¯«ç§’
                }
                Err(_) => {
                    errors += 1;
                }
            }

            if i % 1000 == 0 {
                println!("   å·²å¤„ç†: {}/{} æ¶ˆæ¯", i + 1, self.config.message_count);
            }
        }

        let total_duration = start_time.elapsed();
        self.calculate_results(latencies, errors, total_duration)
    }

    /// æ¶ˆæ¯è·¯ç”±æ€§èƒ½åŸºå‡†æµ‹è¯•
    async fn benchmark_message_routing(&self) -> Result<BenchmarkResults, Box<dyn std::error::Error>> {
        let mut latencies = Vec::new();
        let mut errors = 0;
        let start_time = Instant::now();

        // æ³¨å†Œå¤šä¸ªæµ‹è¯•Agent
        let agent_count = 100;
        for i in 0..agent_count {
            let agent_info = self.create_test_agent_info(i).await;
            let _ = self.a2a_engine.write().await.register_agent(agent_info);
        }

        // æµ‹è¯•æ¶ˆæ¯è·¯ç”±
        for i in 0..self.config.message_count {
            let route_start = Instant::now();
            
            let from_agent = format!("test_agent_{}", i % agent_count);
            let to_agent = format!("test_agent_{}", (i + 1) % agent_count);
            
            match self.route_test_message(&from_agent, &to_agent).await {
                Ok(_) => {
                    let latency = route_start.elapsed();
                    latencies.push(latency.as_secs_f64() * 1000.0);
                }
                Err(_) => {
                    errors += 1;
                }
            }

            if i % 1000 == 0 {
                println!("   å·²è·¯ç”±: {}/{} æ¶ˆæ¯", i + 1, self.config.message_count);
            }
        }

        let total_duration = start_time.elapsed();
        self.calculate_results(latencies, errors, total_duration)
    }

    /// Agentæ³¨å†Œå’Œå‘ç°æ€§èƒ½åŸºå‡†æµ‹è¯•
    async fn benchmark_agent_registry(&self) -> Result<BenchmarkResults, Box<dyn std::error::Error>> {
        let mut latencies = Vec::new();
        let mut errors = 0;
        let start_time = Instant::now();

        // æµ‹è¯•Agentæ³¨å†Œæ€§èƒ½
        for i in 0..self.config.concurrent_agents {
            let reg_start = Instant::now();
            
            let agent_info = self.create_test_agent_info(i).await;
            self.a2a_engine.write().await.register_agent(agent_info);
            let latency = reg_start.elapsed();
            latencies.push(latency.as_secs_f64() * 1000.0);

            if i % 100 == 0 {
                println!("   å·²æ³¨å†Œ: {}/{} Agent", i + 1, self.config.concurrent_agents);
            }
        }

        // æµ‹è¯•Agentå‘ç°æ€§èƒ½
        for i in 0..1000 {
            let discover_start = Instant::now();
            
            match self.a2a_engine.read().await.list_agents() {
                agents => {
                    let latency = discover_start.elapsed();
                    latencies.push(latency.as_secs_f64() * 1000.0);
                    
                    if agents.len() < self.config.concurrent_agents / 2 {
                        errors += 1;
                    }
                }
            }

            if i % 100 == 0 {
                println!("   å·²å‘ç°: {}/1000 æ¬¡æŸ¥è¯¢", i + 1);
            }
        }

        let total_duration = start_time.elapsed();
        self.calculate_results(latencies, errors, total_duration)
    }

    /// é›†ç¾¤ç®¡ç†æ€§èƒ½åŸºå‡†æµ‹è¯•
    async fn benchmark_cluster_management(&self) -> Result<BenchmarkResults, Box<dyn std::error::Error>> {
        let mut latencies = Vec::new();
        let mut errors = 0;
        let start_time = Instant::now();

        // æµ‹è¯•é›†ç¾¤æ“ä½œæ€§èƒ½ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for i in 0..1000 {
            let op_start = Instant::now();

            // æ¨¡æ‹Ÿé›†ç¾¤æ“ä½œ
            match self.cluster_manager.get_cluster_state().await {
                Ok(_) => {
                    let latency = op_start.elapsed();
                    latencies.push(latency.as_secs_f64() * 1000.0);
                }
                Err(_) => {
                    errors += 1;
                }
            }

            if i % 100 == 0 {
                println!("   å·²æ‰§è¡Œ: {}/1000 é›†ç¾¤æ“ä½œ", i + 1);
            }
        }

        let total_duration = start_time.elapsed();
        self.calculate_results(latencies, errors, total_duration)
    }

    /// å¹¶å‘è´Ÿè½½æ€§èƒ½åŸºå‡†æµ‹è¯•
    async fn benchmark_concurrent_load(&self) -> Result<BenchmarkResults, Box<dyn std::error::Error>> {
        let semaphore = Arc::new(Semaphore::new(self.config.concurrent_agents));
        let mut handles = Vec::new();
        let start_time = Instant::now();

        // åˆ›å»ºå¹¶å‘ä»»åŠ¡
        for i in 0..self.config.concurrent_agents {
            let sem = semaphore.clone();
            let engine = self.a2a_engine.clone();
            
            let handle = tokio::spawn(async move {
                let _permit = sem.acquire().await.unwrap();
                let task_start = Instant::now();
                
                // æ‰§è¡Œå¹¶å‘æ“ä½œ
                let agent_info = AgentInfo {
                    id: format!("concurrent_agent_{}", i),
                    name: format!("å¹¶å‘æµ‹è¯•Agent {}", i),
                    endpoint: format!("http://test-{}.local:8080", i),
                    capabilities: vec!["test".to_string()],
                    status: AgentStatus::Online,
                };

                engine.write().await.register_agent(agent_info);
                let latency = task_start.elapsed();

                (true, latency.as_secs_f64() * 1000.0)
            });
            
            handles.push(handle);
        }

        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        let mut latencies = Vec::new();
        let mut errors = 0;
        
        for handle in handles {
            match handle.await {
                Ok((success, latency)) => {
                    if success {
                        latencies.push(latency);
                    } else {
                        errors += 1;
                    }
                }
                Err(_) => {
                    errors += 1;
                }
            }
        }

        let total_duration = start_time.elapsed();
        self.calculate_results(latencies, errors, total_duration)
    }

    /// èµ„æºä½¿ç”¨æ€§èƒ½åŸºå‡†æµ‹è¯•
    async fn benchmark_resource_usage(&self) -> Result<BenchmarkResults, Box<dyn std::error::Error>> {
        // è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        let memory_usage = self.get_memory_usage().await;
        let cpu_usage = self.get_cpu_usage().await;

        Ok(BenchmarkResults {
            avg_latency_ms: 0.0,
            p95_latency_ms: 0.0,
            p99_latency_ms: 0.0,
            throughput_ops_per_sec: 0.0,
            error_rate_percent: 0.0,
            memory_usage_mb: memory_usage,
            cpu_usage_percent: cpu_usage,
        })
    }

    /// å‘é€æµ‹è¯•æ¶ˆæ¯
    async fn send_test_message(&self) -> Result<(), Box<dyn std::error::Error>> {
        let _message = A2AMessage::new_text(
            MessageRole::User,
            "æ€§èƒ½æµ‹è¯•æ¶ˆæ¯".to_string(),
        );
        
        // æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†
        tokio::time::sleep(Duration::from_micros(100)).await;
        Ok(())
    }

    /// è·¯ç”±æµ‹è¯•æ¶ˆæ¯
    async fn route_test_message(&self, from: &str, to: &str) -> Result<(), Box<dyn std::error::Error>> {
        let _message = A2AMessage::new_text(
            MessageRole::User,
            format!("ä» {} åˆ° {} çš„è·¯ç”±æµ‹è¯•", from, to),
        );
        
        // æ¨¡æ‹Ÿæ¶ˆæ¯è·¯ç”±
        tokio::time::sleep(Duration::from_micros(200)).await;
        Ok(())
    }

    /// åˆ›å»ºæµ‹è¯•Agentä¿¡æ¯
    async fn create_test_agent_info(&self, id: usize) -> AgentInfo {
        AgentInfo {
            id: format!("test_agent_{}", id),
            name: format!("æµ‹è¯•Agent {}", id),
            endpoint: format!("http://test-{}.local:8080", id),
            capabilities: vec!["test".to_string(), "benchmark".to_string()],
            status: AgentStatus::Online,
        }
    }

    /// è®¡ç®—æ€§èƒ½æµ‹è¯•ç»“æœ
    fn calculate_results(
        &self,
        mut latencies: Vec<f64>,
        errors: usize,
        total_duration: Duration,
    ) -> Result<BenchmarkResults, Box<dyn std::error::Error>> {
        if latencies.is_empty() {
            return Ok(BenchmarkResults {
                avg_latency_ms: 0.0,
                p95_latency_ms: 0.0,
                p99_latency_ms: 0.0,
                throughput_ops_per_sec: 0.0,
                error_rate_percent: 100.0,
                memory_usage_mb: 0.0,
                cpu_usage_percent: 0.0,
            });
        }

        latencies.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        let avg_latency = latencies.iter().sum::<f64>() / latencies.len() as f64;
        let p95_index = (latencies.len() as f64 * 0.95) as usize;
        let p99_index = (latencies.len() as f64 * 0.99) as usize;
        
        let p95_latency = latencies[p95_index.min(latencies.len() - 1)];
        let p99_latency = latencies[p99_index.min(latencies.len() - 1)];
        
        let total_ops = latencies.len() + errors;
        let throughput = total_ops as f64 / total_duration.as_secs_f64();
        let error_rate = (errors as f64 / total_ops as f64) * 100.0;

        Ok(BenchmarkResults {
            avg_latency_ms: avg_latency,
            p95_latency_ms: p95_latency,
            p99_latency_ms: p99_latency,
            throughput_ops_per_sec: throughput,
            error_rate_percent: error_rate,
            memory_usage_mb: 0.0,
            cpu_usage_percent: 0.0,
        })
    }

    /// è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ (MB)
    async fn get_memory_usage(&self) -> f64 {
        // ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ç³»ç»ŸAPI
        128.0 // æ¨¡æ‹Ÿå€¼
    }

    /// è·å–CPUä½¿ç”¨ç‡ (%)
    async fn get_cpu_usage(&self) -> f64 {
        // ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ç³»ç»ŸAPI
        15.0 // æ¨¡æ‹Ÿå€¼
    }

    /// æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦
    fn print_summary(&self, results: &[BenchmarkResults]) {
        println!("\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœæ‘˜è¦");
        println!("{}", "=".repeat(60));
        
        let test_names = [
            "A2Aåè®®æ€§èƒ½",
            "æ¶ˆæ¯è·¯ç”±æ€§èƒ½", 
            "Agentæ³¨å†Œå‘ç°",
            "é›†ç¾¤ç®¡ç†æ€§èƒ½",
            "å¹¶å‘è´Ÿè½½æµ‹è¯•",
            "èµ„æºä½¿ç”¨æƒ…å†µ",
        ];

        for (i, result) in results.iter().enumerate() {
            if i < test_names.len() {
                println!("\nğŸ”¸ {}", test_names[i]);
                println!("   å¹³å‡å»¶è¿Ÿ: {:.2}ms", result.avg_latency_ms);
                println!("   P95å»¶è¿Ÿ: {:.2}ms", result.p95_latency_ms);
                println!("   P99å»¶è¿Ÿ: {:.2}ms", result.p99_latency_ms);
                println!("   ååé‡: {:.0} ops/sec", result.throughput_ops_per_sec);
                println!("   é”™è¯¯ç‡: {:.2}%", result.error_rate_percent);
                
                if result.memory_usage_mb > 0.0 {
                    println!("   å†…å­˜ä½¿ç”¨: {:.1}MB", result.memory_usage_mb);
                }
                if result.cpu_usage_percent > 0.0 {
                    println!("   CPUä½¿ç”¨: {:.1}%", result.cpu_usage_percent);
                }
            }
        }

        println!("\nğŸ¯ æ€§èƒ½ç›®æ ‡éªŒè¯:");
        self.validate_performance_targets(results);
    }

    /// éªŒè¯æ€§èƒ½ç›®æ ‡
    fn validate_performance_targets(&self, results: &[BenchmarkResults]) {
        let mut all_passed = true;

        // éªŒè¯æ¶ˆæ¯è·¯ç”±å»¶è¿Ÿ < 10ms
        if let Some(routing_result) = results.get(1) {
            let passed = routing_result.avg_latency_ms < 10.0;
            println!("   æ¶ˆæ¯è·¯ç”±å»¶è¿Ÿ < 10ms: {} ({:.2}ms)", 
                if passed { "âœ… é€šè¿‡" } else { "âŒ å¤±è´¥" }, 
                routing_result.avg_latency_ms);
            all_passed &= passed;
        }

        // éªŒè¯Agentæ³¨å†Œæ€§èƒ½ > 1000 ops/sec
        if let Some(registry_result) = results.get(2) {
            let passed = registry_result.throughput_ops_per_sec > 1000.0;
            println!("   Agentæ³¨å†Œååé‡ > 1000 ops/sec: {} ({:.0} ops/sec)", 
                if passed { "âœ… é€šè¿‡" } else { "âŒ å¤±è´¥" }, 
                registry_result.throughput_ops_per_sec);
            all_passed &= passed;
        }

        // éªŒè¯å¹¶å‘æ”¯æŒ > 1000 Agent
        if let Some(concurrent_result) = results.get(4) {
            let passed = concurrent_result.error_rate_percent < 5.0;
            println!("   å¹¶å‘Agentæ”¯æŒ: {} (é”™è¯¯ç‡: {:.2}%)", 
                if passed { "âœ… é€šè¿‡" } else { "âŒ å¤±è´¥" }, 
                concurrent_result.error_rate_percent);
            all_passed &= passed;
        }

        println!("\nğŸ† æ€»ä½“è¯„ä¼°: {}", 
            if all_passed { "âœ… æ‰€æœ‰æ€§èƒ½ç›®æ ‡è¾¾æˆ" } else { "âŒ éƒ¨åˆ†æ€§èƒ½ç›®æ ‡æœªè¾¾æˆ" });
    }
}

#[tokio::test]
async fn test_performance_benchmarks() {
    let config = BenchmarkConfig::default();
    let benchmarks = PerformanceBenchmarks::new(config).await.unwrap();
    
    let results = benchmarks.run_full_benchmark().await.unwrap();
    assert!(!results.is_empty(), "åº”è¯¥æœ‰æ€§èƒ½æµ‹è¯•ç»“æœ");
    
    // éªŒè¯å…³é”®æ€§èƒ½æŒ‡æ ‡
    for result in &results {
        assert!(result.avg_latency_ms >= 0.0, "å¹³å‡å»¶è¿Ÿåº”è¯¥ä¸ºéè´Ÿæ•°");
        assert!(result.throughput_ops_per_sec >= 0.0, "ååé‡åº”è¯¥ä¸ºéè´Ÿæ•°");
        assert!(result.error_rate_percent >= 0.0 && result.error_rate_percent <= 100.0, 
            "é”™è¯¯ç‡åº”è¯¥åœ¨0-100%ä¹‹é—´");
    }
}
