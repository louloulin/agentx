//! LangChainæ’ä»¶æ¼”ç¤º
//! 
//! å±•ç¤ºå¦‚ä½•ä½¿ç”¨AgentX SDKåˆ›å»ºå’Œç®¡ç†LangChainæ’ä»¶

use agentx_sdk::{
    init_sdk, PluginBuilder, FrameworkBuilder, ConfigBuilder,
    FrameworkType, PluginCapability, PluginUtils, FrameworkUtils,
    A2AMessage, MessageRole, A2AResult,
};
use std::collections::HashMap;
use tokio;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ðŸ LangChainæ’ä»¶æ¼”ç¤º");
    println!("====================");
    
    // 1. åˆå§‹åŒ–SDK
    println!("\nðŸ“¦ 1. åˆå§‹åŒ–AgentX SDK");
    init_sdk().await?;
    
    // 2. æ£€æµ‹PythonçŽ¯å¢ƒ
    println!("\nðŸ” 2. æ£€æµ‹PythonçŽ¯å¢ƒ");
    let env_info = FrameworkUtils::detect_framework_environment("langchain").await?;
    println!("   è¿è¡Œæ—¶: {}", env_info.runtime);
    println!("   ç‰ˆæœ¬: {}", env_info.version);
    println!("   å¯ç”¨: {}", env_info.available);
    
    if !env_info.available {
        println!("   âš ï¸  PythonçŽ¯å¢ƒä¸å¯ç”¨ï¼Œæ¼”ç¤ºå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼");
    }
    
    // 3. åˆ›å»ºLangChainé…ç½®
    println!("\nâš™ï¸  3. åˆ›å»ºLangChainé…ç½®");
    let config = ConfigBuilder::new()
        .framework("langchain")
        .runtime_path("python")
        .working_directory("./examples/langchain")
        .env_var("PYTHONPATH", ".")
        .env_var("OPENAI_API_KEY", "your-api-key-here")
        .custom("model", serde_json::Value::String("gpt-3.5-turbo".to_string()))
        .custom("temperature", serde_json::Value::Number(serde_json::Number::from_f64(0.7).unwrap()))
        .build()?;
    
    println!("   æ¡†æž¶: {}", config.framework);
    println!("   ç»‘å®šåœ°å€: {}", config.bind_address);
    println!("   æœåŠ¡å™¨åœ°å€: {}", config.server_address);
    
    // 4. æž„å»ºLangChainæ’ä»¶
    println!("\nðŸ”§ 4. æž„å»ºLangChainæ’ä»¶");
    let plugin = PluginBuilder::new()
        .framework("langchain")
        .config(config)
        .capability(PluginCapability::TextProcessing)
        .capability(PluginCapability::ToolCalling)
        .capability(PluginCapability::KnowledgeRetrieval)
        .build()
        .await?;
    
    println!("   æ’ä»¶ID: {}", plugin.get_info().metadata.id);
    println!("   æ’ä»¶åç§°: {}", plugin.get_info().metadata.name);
    println!("   æ’ä»¶ç‰ˆæœ¬: {}", plugin.get_info().metadata.version);
    println!("   æ”¯æŒèƒ½åŠ›: {:?}", plugin.get_capabilities());
    
    // 5. åˆ›å»ºLangChainæ¡†æž¶å®žä¾‹
    println!("\nðŸ—ï¸  5. åˆ›å»ºLangChainæ¡†æž¶å®žä¾‹");
    let framework_config = agentx_sdk::framework::FrameworkConfig {
        framework_type: FrameworkType::LangChain,
        runtime_path: "python".to_string(),
        working_directory: "./examples/langchain".to_string(),
        environment_variables: {
            let mut env = HashMap::new();
            env.insert("PYTHONPATH".to_string(), ".".to_string());
            env.insert("OPENAI_API_KEY".to_string(), "your-api-key-here".to_string());
            env
        },
        startup_args: vec![
            "-c".to_string(),
            "import langchain; print('LangChain loaded successfully')".to_string(),
        ],
        dependencies: vec![
            "langchain".to_string(),
            "langchain-community".to_string(),
            "langchain-openai".to_string(),
        ],
        custom_config: HashMap::new(),
    };
    
    let mut framework = FrameworkBuilder::new()
        .framework_type(FrameworkType::LangChain)
        .config(framework_config)
        .build()
        .await?;
    
    println!("   æ¡†æž¶ç±»åž‹: {:?}", framework.get_type());
    println!("   è¿è¡ŒçŠ¶æ€: {}", framework.is_running());
    
    // 6. æ¼”ç¤ºæ¶ˆæ¯å¤„ç†
    println!("\nðŸ’¬ 6. æ¼”ç¤ºæ¶ˆæ¯å¤„ç†");
    
    // åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
    let test_messages = vec![
        A2AMessage::new_text(MessageRole::User, "Hello, LangChain!".to_string()),
        A2AMessage::new_text(MessageRole::User, "What is artificial intelligence?".to_string()),
        A2AMessage::new_text(MessageRole::User, "Explain machine learning in simple terms.".to_string()),
    ];
    
    for (i, message) in test_messages.iter().enumerate() {
        println!("   ðŸ“¨ å¤„ç†æ¶ˆæ¯ {}: {}", i + 1, 
            agentx_sdk::MessageUtils::extract_text_content(message));
        
        // éªŒè¯æ¶ˆæ¯
        agentx_sdk::MessageUtils::validate_message(message)?;
        
        // è®¡ç®—æ¶ˆæ¯å¤§å°
        let size = agentx_sdk::MessageUtils::calculate_message_size(message);
        println!("     æ¶ˆæ¯å¤§å°: {} å­—èŠ‚", size);
        
        // æ¨¡æ‹Ÿæ¡†æž¶å¤„ç†ï¼ˆå®žé™…çŽ¯å¢ƒä¸­ä¼šè°ƒç”¨çœŸå®žçš„LangChainï¼‰
        if env_info.available {
            match framework.process_message(message.clone()).await {
                Ok(Some(response)) => {
                    println!("     âœ… å“åº”: {}", 
                        agentx_sdk::MessageUtils::extract_text_content(&response));
                },
                Ok(None) => {
                    println!("     â„¹ï¸  æ— å“åº”");
                },
                Err(e) => {
                    println!("     âŒ å¤„ç†å¤±è´¥: {:?}", e);
                }
            }
        } else {
            println!("     ðŸ”„ æ¨¡æ‹Ÿå“åº”: Processed by LangChain (simulated)");
        }
    }
    
    // 7. æ¼”ç¤ºå·¥å…·è°ƒç”¨
    println!("\nðŸ› ï¸  7. æ¼”ç¤ºå·¥å…·è°ƒç”¨");
    
    let tool_call_data = serde_json::json!({
        "tool_name": "web_search",
        "arguments": {
            "query": "latest AI news",
            "max_results": 5
        },
        "call_id": "tool_call_001"
    });
    
    let tool_message = A2AMessage::new_data(MessageRole::User, tool_call_data);
    println!("   ðŸ”§ å·¥å…·è°ƒç”¨æ¶ˆæ¯: {}", tool_message.message_id);
    
    if env_info.available {
        match framework.process_message(tool_message).await {
            Ok(Some(response)) => {
                println!("   âœ… å·¥å…·è°ƒç”¨å“åº”: {}", 
                    agentx_sdk::MessageUtils::extract_text_content(&response));
            },
            Ok(None) => {
                println!("   â„¹ï¸  å·¥å…·è°ƒç”¨æ— å“åº”");
            },
            Err(e) => {
                println!("   âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {:?}", e);
            }
        }
    } else {
        println!("   ðŸ”„ æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨å“åº”: Web search completed (simulated)");
    }
    
    // 8. æ¼”ç¤ºæ’ä»¶ç»Ÿè®¡
    println!("\nðŸ“Š 8. æ’ä»¶ç»Ÿè®¡ä¿¡æ¯");
    let stats = plugin.get_stats();
    println!("   å¤„ç†æ¶ˆæ¯æ•°: {}", stats.messages_processed);
    println!("   å‘é€æ¶ˆæ¯æ•°: {}", stats.messages_sent);
    println!("   æŽ¥æ”¶æ¶ˆæ¯æ•°: {}", stats.messages_received);
    println!("   é”™è¯¯æ•°: {}", stats.errors);
    println!("   å¹³å‡å“åº”æ—¶é—´: {:.2}ms", stats.avg_response_time_ms);
    
    // 9. æ¼”ç¤ºé…ç½®ç®¡ç†
    println!("\nâš™ï¸  9. é…ç½®ç®¡ç†æ¼”ç¤º");
    
    // ä»ŽçŽ¯å¢ƒå˜é‡åŠ è½½é…ç½®
    let env_config = agentx_sdk::ConfigUtils::load_from_env();
    println!("   çŽ¯å¢ƒé…ç½®æ¡†æž¶: {}", env_config.framework);
    
    // åˆå¹¶é…ç½®
    let merged_config = agentx_sdk::ConfigUtils::merge_configs(
        plugin.get_info().config.clone(),
        env_config
    );
    println!("   åˆå¹¶åŽæ¡†æž¶: {}", merged_config.framework);
    
    // 10. æ¸…ç†èµ„æº
    println!("\nðŸ§¹ 10. æ¸…ç†èµ„æº");
    
    if framework.is_running() {
        framework.stop().await?;
        println!("   âœ… LangChainæ¡†æž¶å·²åœæ­¢");
    }
    
    println!("   âœ… èµ„æºæ¸…ç†å®Œæˆ");
    
    // 11. æ€»ç»“
    println!("\nðŸ“‹ 11. æ¼”ç¤ºæ€»ç»“");
    println!("   âœ… SDKåˆå§‹åŒ–æˆåŠŸ");
    println!("   âœ… PythonçŽ¯å¢ƒæ£€æµ‹å®Œæˆ");
    println!("   âœ… LangChainé…ç½®åˆ›å»ºæˆåŠŸ");
    println!("   âœ… æ’ä»¶æž„å»ºæˆåŠŸ");
    println!("   âœ… æ¡†æž¶å®žä¾‹åˆ›å»ºæˆåŠŸ");
    println!("   âœ… æ¶ˆæ¯å¤„ç†æ¼”ç¤ºå®Œæˆ");
    println!("   âœ… å·¥å…·è°ƒç”¨æ¼”ç¤ºå®Œæˆ");
    println!("   âœ… ç»Ÿè®¡ä¿¡æ¯æŸ¥çœ‹å®Œæˆ");
    println!("   âœ… é…ç½®ç®¡ç†æ¼”ç¤ºå®Œæˆ");
    println!("   âœ… èµ„æºæ¸…ç†å®Œæˆ");
    
    println!("\nðŸŽ‰ LangChainæ’ä»¶æ¼”ç¤ºå®Œæˆï¼");
    println!("=====================================");
    
    Ok(())
}

/// åˆ›å»ºç¤ºä¾‹LangChainè„šæœ¬
async fn create_example_script() -> A2AResult<()> {
    let script_content = r#"
#!/usr/bin/env python3
"""
LangChain AgentX æ’ä»¶ç¤ºä¾‹è„šæœ¬
"""

import sys
import json
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def process_message(message_data):
    """å¤„ç†A2Aæ¶ˆæ¯"""
    try:
        # è§£æžæ¶ˆæ¯
        message = json.loads(message_data)
        content = message.get('content', '')
        
        # åˆ›å»ºLangChainç»„ä»¶
        llm = OpenAI(temperature=0.7)
        prompt = PromptTemplate(
            input_variables=["question"],
            template="You are a helpful AI assistant. Answer the following question: {question}"
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        
        # å¤„ç†æ¶ˆæ¯
        response = chain.run(question=content)
        
        # è¿”å›žå“åº”
        return {
            "type": "response",
            "content": response,
            "status": "success"
        }
        
    except Exception as e:
        return {
            "type": "error",
            "content": str(e),
            "status": "error"
        }

if __name__ == "__main__":
    if len(sys.argv) > 1:
        message_data = sys.argv[1]
        result = process_message(message_data)
        print(json.dumps(result))
    else:
        print("LangChain AgentX Plugin Ready")
"#;
    
    // åˆ›å»ºç¤ºä¾‹ç›®å½•
    tokio::fs::create_dir_all("./examples/langchain").await
        .map_err(|e| agentx_sdk::A2AError::internal(format!("åˆ›å»ºç›®å½•å¤±è´¥: {}", e)))?;
    
    // å†™å…¥è„šæœ¬æ–‡ä»¶
    tokio::fs::write("./examples/langchain/plugin.py", script_content).await
        .map_err(|e| agentx_sdk::A2AError::internal(format!("å†™å…¥è„šæœ¬å¤±è´¥: {}", e)))?;
    
    println!("   âœ… ç¤ºä¾‹è„šæœ¬å·²åˆ›å»º: ./examples/langchain/plugin.py");
    
    Ok(())
}
