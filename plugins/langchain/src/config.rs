//! LangChainæ’ä»¶é…ç½®
//! 
//! ç®¡ç†LangChainæ’ä»¶çš„é…ç½®å‚æ•°

use crate::error::{LangChainError, LangChainResult};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use tracing::{info, debug};

/// LangChainæ’ä»¶é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LangChainConfig {
    /// æ’ä»¶æœåŠ¡å™¨é…ç½®
    pub host: String,
    pub port: u16,
    
    /// Pythonç¯å¢ƒé…ç½®
    pub python_executable: String,
    pub python_service_host: String,
    pub python_service_port: u16,
    pub python_path: Vec<String>,
    pub environment_variables: HashMap<String, String>,
    
    /// LangChainé…ç½®
    pub default_model: String,
    pub api_keys: HashMap<String, String>,
    pub model_configs: HashMap<String, ModelConfig>,
    pub tool_configs: HashMap<String, ToolConfig>,
    
    /// æ€§èƒ½é…ç½®
    pub max_concurrent_requests: usize,
    pub request_timeout_seconds: u64,
    pub memory_limit_mb: usize,
    pub cache_size: usize,
    
    /// æ—¥å¿—é…ç½®
    pub log_level: String,
    pub log_file: Option<PathBuf>,
    pub enable_metrics: bool,
    
    /// å®‰å…¨é…ç½®
    pub enable_authentication: bool,
    pub api_key: Option<String>,
    pub allowed_origins: Vec<String>,
}

/// æ¨¡å‹é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelConfig {
    pub provider: String,
    pub model_name: String,
    pub api_key_env: Option<String>,
    pub base_url: Option<String>,
    pub max_tokens: Option<u32>,
    pub temperature: Option<f32>,
    pub parameters: HashMap<String, serde_json::Value>,
}

/// å·¥å…·é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ToolConfig {
    pub tool_type: String,
    pub enabled: bool,
    pub config: HashMap<String, serde_json::Value>,
    pub dependencies: Vec<String>,
}

impl Default for LangChainConfig {
    fn default() -> Self {
        let mut api_keys = HashMap::new();
        let mut environment_variables = HashMap::new();
        let mut model_configs = HashMap::new();
        let mut tool_configs = HashMap::new();
        
        // é»˜è®¤ç¯å¢ƒå˜é‡
        environment_variables.insert("PYTHONPATH".to_string(), ".".to_string());
        environment_variables.insert("LANGCHAIN_TRACING_V2".to_string(), "false".to_string());
        
        // é»˜è®¤æ¨¡å‹é…ç½®
        model_configs.insert("gpt-3.5-turbo".to_string(), ModelConfig {
            provider: "openai".to_string(),
            model_name: "gpt-3.5-turbo".to_string(),
            api_key_env: Some("OPENAI_API_KEY".to_string()),
            base_url: None,
            max_tokens: Some(4096),
            temperature: Some(0.7),
            parameters: HashMap::new(),
        });
        
        model_configs.insert("gpt-4".to_string(), ModelConfig {
            provider: "openai".to_string(),
            model_name: "gpt-4".to_string(),
            api_key_env: Some("OPENAI_API_KEY".to_string()),
            base_url: None,
            max_tokens: Some(8192),
            temperature: Some(0.7),
            parameters: HashMap::new(),
        });
        
        model_configs.insert("claude-3-sonnet".to_string(), ModelConfig {
            provider: "anthropic".to_string(),
            model_name: "claude-3-sonnet-20240229".to_string(),
            api_key_env: Some("ANTHROPIC_API_KEY".to_string()),
            base_url: None,
            max_tokens: Some(4096),
            temperature: Some(0.7),
            parameters: HashMap::new(),
        });
        
        // é»˜è®¤å·¥å…·é…ç½®
        tool_configs.insert("search".to_string(), ToolConfig {
            tool_type: "web_search".to_string(),
            enabled: true,
            config: HashMap::new(),
            dependencies: vec!["requests".to_string(), "beautifulsoup4".to_string()],
        });
        
        tool_configs.insert("calculator".to_string(), ToolConfig {
            tool_type: "math".to_string(),
            enabled: true,
            config: HashMap::new(),
            dependencies: vec!["sympy".to_string()],
        });
        
        Self {
            host: "0.0.0.0".to_string(),
            port: 50052,
            python_executable: "python3".to_string(),
            python_service_host: "127.0.0.1".to_string(),
            python_service_port: 8000,
            python_path: vec![".".to_string()],
            environment_variables,
            default_model: "gpt-3.5-turbo".to_string(),
            api_keys,
            model_configs,
            tool_configs,
            max_concurrent_requests: 100,
            request_timeout_seconds: 300,
            memory_limit_mb: 1024,
            cache_size: 1000,
            log_level: "info".to_string(),
            log_file: None,
            enable_metrics: true,
            enable_authentication: false,
            api_key: None,
            allowed_origins: vec!["*".to_string()],
        }
    }
}

impl LangChainConfig {
    /// ä»æ–‡ä»¶åŠ è½½é…ç½®
    pub async fn load() -> LangChainResult<Self> {
        info!("ğŸ“‹ åŠ è½½LangChainæ’ä»¶é…ç½®...");
        
        // å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½é…ç½®æ–‡ä»¶
        let config_paths = vec![
            "langchain_config.toml",
            "config/langchain.toml",
            "/etc/agentx/langchain.toml",
            "~/.agentx/langchain.toml",
        ];
        
        for path in config_paths {
            if let Ok(config) = Self::load_from_file(path).await {
                info!("âœ… ä» {} åŠ è½½é…ç½®æˆåŠŸ", path);
                return Ok(config);
            }
        }
        
        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        info!("ğŸ“„ ä½¿ç”¨é»˜è®¤é…ç½®");
        let mut config = Self::default();
        
        // ä»ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®
        config.load_from_env();
        
        Ok(config)
    }
    
    /// ä»æ–‡ä»¶åŠ è½½é…ç½®
    async fn load_from_file(path: &str) -> LangChainResult<Self> {
        let content = tokio::fs::read_to_string(path).await
            .map_err(|e| LangChainError::ConfigError(format!("è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {}", e)))?;
        
        let config: Self = toml::from_str(&content)
            .map_err(|e| LangChainError::ConfigError(format!("è§£æé…ç½®æ–‡ä»¶å¤±è´¥: {}", e)))?;
        
        debug!("ä»æ–‡ä»¶åŠ è½½é…ç½®: {}", path);
        Ok(config)
    }
    
    /// ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
    fn load_from_env(&mut self) {
        debug!("ğŸŒ ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®...");
        
        // æœåŠ¡å™¨é…ç½®
        if let Ok(host) = std::env::var("LANGCHAIN_HOST") {
            self.host = host;
        }
        if let Ok(port) = std::env::var("LANGCHAIN_PORT") {
            if let Ok(port) = port.parse() {
                self.port = port;
            }
        }
        
        // Pythoné…ç½®
        if let Ok(python) = std::env::var("PYTHON_EXECUTABLE") {
            self.python_executable = python;
        }
        if let Ok(host) = std::env::var("PYTHON_SERVICE_HOST") {
            self.python_service_host = host;
        }
        if let Ok(port) = std::env::var("PYTHON_SERVICE_PORT") {
            if let Ok(port) = port.parse() {
                self.python_service_port = port;
            }
        }
        
        // APIå¯†é’¥
        if let Ok(openai_key) = std::env::var("OPENAI_API_KEY") {
            self.api_keys.insert("openai".to_string(), openai_key);
        }
        if let Ok(anthropic_key) = std::env::var("ANTHROPIC_API_KEY") {
            self.api_keys.insert("anthropic".to_string(), anthropic_key);
        }
        
        // é»˜è®¤æ¨¡å‹
        if let Ok(model) = std::env::var("DEFAULT_MODEL") {
            self.default_model = model;
        }
        
        // æ€§èƒ½é…ç½®
        if let Ok(max_requests) = std::env::var("MAX_CONCURRENT_REQUESTS") {
            if let Ok(max_requests) = max_requests.parse() {
                self.max_concurrent_requests = max_requests;
            }
        }
        
        // æ—¥å¿—é…ç½®
        if let Ok(log_level) = std::env::var("LOG_LEVEL") {
            self.log_level = log_level;
        }
        
        debug!("âœ… ç¯å¢ƒå˜é‡é…ç½®åŠ è½½å®Œæˆ");
    }
    
    /// ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    pub async fn save_to_file(&self, path: &str) -> LangChainResult<()> {
        let content = toml::to_string_pretty(self)
            .map_err(|e| LangChainError::ConfigError(format!("åºåˆ—åŒ–é…ç½®å¤±è´¥: {}", e)))?;
        
        tokio::fs::write(path, content).await
            .map_err(|e| LangChainError::ConfigError(format!("å†™å…¥é…ç½®æ–‡ä»¶å¤±è´¥: {}", e)))?;
        
        info!("ğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {}", path);
        Ok(())
    }
    
    /// éªŒè¯é…ç½®
    pub fn validate(&self) -> LangChainResult<()> {
        debug!("ğŸ” éªŒè¯é…ç½®...");
        
        // éªŒè¯ç«¯å£èŒƒå›´
        if self.port == 0 || self.port > 65535 {
            return Err(LangChainError::ConfigError(
                format!("æ— æ•ˆçš„ç«¯å£å·: {}", self.port)
            ));
        }
        
        if self.python_service_port == 0 || self.python_service_port > 65535 {
            return Err(LangChainError::ConfigError(
                format!("æ— æ•ˆçš„PythonæœåŠ¡ç«¯å£å·: {}", self.python_service_port)
            ));
        }
        
        // éªŒè¯Pythonå¯æ‰§è¡Œæ–‡ä»¶
        if self.python_executable.is_empty() {
            return Err(LangChainError::ConfigError(
                "Pythonå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º".to_string()
            ));
        }
        
        // éªŒè¯é»˜è®¤æ¨¡å‹
        if !self.model_configs.contains_key(&self.default_model) {
            return Err(LangChainError::ConfigError(
                format!("é»˜è®¤æ¨¡å‹ '{}' æœªåœ¨æ¨¡å‹é…ç½®ä¸­å®šä¹‰", self.default_model)
            ));
        }
        
        // éªŒè¯æ€§èƒ½å‚æ•°
        if self.max_concurrent_requests == 0 {
            return Err(LangChainError::ConfigError(
                "æœ€å¤§å¹¶å‘è¯·æ±‚æ•°å¿…é¡»å¤§äº0".to_string()
            ));
        }
        
        if self.request_timeout_seconds == 0 {
            return Err(LangChainError::ConfigError(
                "è¯·æ±‚è¶…æ—¶æ—¶é—´å¿…é¡»å¤§äº0".to_string()
            ));
        }
        
        debug!("âœ… é…ç½®éªŒè¯é€šè¿‡");
        Ok(())
    }
    
    /// è®¾ç½®å‚æ•°
    pub fn set_parameter(&mut self, key: &str, value: &str) {
        match key {
            "host" => self.host = value.to_string(),
            "port" => {
                if let Ok(port) = value.parse() {
                    self.port = port;
                }
            }
            "python_executable" => self.python_executable = value.to_string(),
            "default_model" => self.default_model = value.to_string(),
            "log_level" => self.log_level = value.to_string(),
            _ => {
                // æ·»åŠ åˆ°ç¯å¢ƒå˜é‡
                self.environment_variables.insert(key.to_string(), value.to_string());
            }
        }
    }
    
    /// è·å–è¿è¡Œæ—¶ä¿¡æ¯
    pub fn get_runtime_info(&self) -> HashMap<String, String> {
        let mut info = HashMap::new();
        
        info.insert("host".to_string(), self.host.clone());
        info.insert("port".to_string(), self.port.to_string());
        info.insert("python_executable".to_string(), self.python_executable.clone());
        info.insert("python_service_host".to_string(), self.python_service_host.clone());
        info.insert("python_service_port".to_string(), self.python_service_port.to_string());
        info.insert("default_model".to_string(), self.default_model.clone());
        info.insert("max_concurrent_requests".to_string(), self.max_concurrent_requests.to_string());
        info.insert("log_level".to_string(), self.log_level.clone());
        
        info
    }
    
    /// è·å–æ¨¡å‹é…ç½®
    pub fn get_model_config(&self, model_name: &str) -> Option<&ModelConfig> {
        self.model_configs.get(model_name)
    }
    
    /// è·å–å·¥å…·é…ç½®
    pub fn get_tool_config(&self, tool_name: &str) -> Option<&ToolConfig> {
        self.tool_configs.get(tool_name)
    }
    
    /// è·å–APIå¯†é’¥
    pub fn get_api_key(&self, provider: &str) -> Option<&String> {
        self.api_keys.get(provider)
    }
}
