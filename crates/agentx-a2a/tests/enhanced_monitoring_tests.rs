//! å¢å¼ºç›‘æ§åŠŸèƒ½æµ‹è¯•
//! 
//! æµ‹è¯•æ–°å¢çš„æ€§èƒ½ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•åŠŸèƒ½

use agentx_a2a::{
    MonitoringManager, MonitoringConfig, MetricPoint, MetricType,
    BenchmarkResult, PerformanceStats
};
use chrono::Utc;
use std::collections::HashMap;
use std::time::Duration;

/// åˆ›å»ºæµ‹è¯•ç”¨çš„ç›‘æ§é…ç½®
fn create_test_config() -> MonitoringConfig {
    MonitoringConfig {
        metric_retention_hours: 24,
        health_check_interval_seconds: 30,
        stats_calculation_interval_seconds: 60,
        enable_detailed_monitoring: true,
    }
}

#[tokio::test]
async fn test_enhanced_performance_stats() {
    println!("ğŸ§ª æµ‹è¯•å¢å¼ºæ€§èƒ½ç»Ÿè®¡åŠŸèƒ½");
    
    let config = create_test_config();
    let mut manager = MonitoringManager::new(config);
    
    // æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
    manager.increment_counter("total_messages", 1000);
    manager.increment_counter("successful_messages", 950);
    manager.increment_counter("failed_messages", 50);
    
    // æ·»åŠ å»¶è¿ŸæŒ‡æ ‡
    let latency_metric = MetricPoint {
        name: "message_latency".to_string(),
        metric_type: MetricType::Histogram,
        value: 5.5, // 5.5ms
        labels: HashMap::new(),
        timestamp: Utc::now(),
        help: Some("æ¶ˆæ¯å¤„ç†å»¶è¿Ÿ".to_string()),
    };
    manager.record_metric(latency_metric);
    
    // è·å–å¢å¼ºæ€§èƒ½ç»Ÿè®¡
    let stats = manager.get_enhanced_performance_stats();
    
    // éªŒè¯ç»Ÿè®¡æ•°æ®
    assert_eq!(stats.message_stats.total_messages, 1000);
    assert_eq!(stats.message_stats.successful_messages, 950);
    assert_eq!(stats.message_stats.failed_messages, 50);
    assert_eq!(stats.error_stats.error_rate, 5.0); // 50/1000 * 100 = 5%
    
    println!("âœ… å¢å¼ºæ€§èƒ½ç»Ÿè®¡æµ‹è¯•é€šè¿‡");
    println!("   - æ€»æ¶ˆæ¯æ•°: {}", stats.message_stats.total_messages);
    println!("   - æˆåŠŸç‡: {:.1}%", 100.0 - stats.error_stats.error_rate);
    println!("   - å¹³å‡å»¶è¿Ÿ: {:.2}ms", stats.message_stats.avg_processing_time_ms);
}

#[tokio::test]
async fn test_performance_benchmark() {
    println!("ğŸ§ª æµ‹è¯•æ€§èƒ½åŸºå‡†æµ‹è¯•åŠŸèƒ½");
    
    let config = create_test_config();
    let mut manager = MonitoringManager::new(config);
    
    // è¿è¡Œå°è§„æ¨¡åŸºå‡†æµ‹è¯•
    let iterations = 1000;
    let result = manager.run_performance_benchmark("test_benchmark", iterations);
    
    // éªŒè¯åŸºå‡†æµ‹è¯•ç»“æœ
    assert_eq!(result.test_name, "test_benchmark");
    assert_eq!(result.iterations, iterations);
    assert!(result.total_duration_ms > 0.0);
    assert!(result.throughput_ops_per_sec > 0.0);
    assert!(result.min_latency_ms >= 0.0);
    assert!(result.max_latency_ms >= result.min_latency_ms);
    assert!(result.avg_latency_ms >= result.min_latency_ms);
    assert!(result.avg_latency_ms <= result.max_latency_ms);
    assert!(result.p50_latency_ms >= result.min_latency_ms);
    assert!(result.p95_latency_ms >= result.p50_latency_ms);
    assert!(result.p99_latency_ms >= result.p95_latency_ms);
    
    println!("âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡");
    println!("   - è¿­ä»£æ¬¡æ•°: {}", result.iterations);
    println!("   - æ€»è€—æ—¶: {:.2}ms", result.total_duration_ms);
    println!("   - ååé‡: {:.0} ops/sec", result.throughput_ops_per_sec);
    println!("   - å¹³å‡å»¶è¿Ÿ: {:.3}ms", result.avg_latency_ms);
    println!("   - P95å»¶è¿Ÿ: {:.3}ms", result.p95_latency_ms);
    println!("   - P99å»¶è¿Ÿ: {:.3}ms", result.p99_latency_ms);
}

#[tokio::test]
async fn test_metric_recording_and_retrieval() {
    println!("ğŸ§ª æµ‹è¯•æŒ‡æ ‡è®°å½•å’Œæ£€ç´¢åŠŸèƒ½");
    
    let config = create_test_config();
    let mut manager = MonitoringManager::new(config);
    
    // è®°å½•å¤šç§ç±»å‹çš„æŒ‡æ ‡
    let counter_metric = MetricPoint {
        name: "api_requests".to_string(),
        metric_type: MetricType::Counter,
        value: 100.0,
        labels: HashMap::from([("endpoint".to_string(), "/api/v1/agents".to_string())]),
        timestamp: Utc::now(),
        help: Some("APIè¯·æ±‚è®¡æ•°".to_string()),
    };
    manager.record_metric(counter_metric);
    
    let gauge_metric = MetricPoint {
        name: "memory_usage".to_string(),
        metric_type: MetricType::Gauge,
        value: 85.5,
        labels: HashMap::from([("unit".to_string(), "percent".to_string())]),
        timestamp: Utc::now(),
        help: Some("å†…å­˜ä½¿ç”¨ç‡".to_string()),
    };
    manager.record_metric(gauge_metric);
    
    let histogram_metric = MetricPoint {
        name: "response_time".to_string(),
        metric_type: MetricType::Histogram,
        value: 125.7,
        labels: HashMap::from([("service".to_string(), "agent_registry".to_string())]),
        timestamp: Utc::now(),
        help: Some("å“åº”æ—¶é—´åˆ†å¸ƒ".to_string()),
    };
    manager.record_metric(histogram_metric);
    
    // å¢åŠ è®¡æ•°å™¨
    manager.increment_counter("total_operations", 50);
    manager.increment_counter("total_operations", 25);
    
    // è®¾ç½®ä»ªè¡¨å€¼
    let mut labels = HashMap::new();
    labels.insert("component".to_string(), "a2a_engine".to_string());
    manager.set_gauge("cpu_usage", 45.2, labels);
    
    println!("âœ… æŒ‡æ ‡è®°å½•å’Œæ£€ç´¢æµ‹è¯•é€šè¿‡");
    println!("   - å·²è®°å½•å¤šç§ç±»å‹æŒ‡æ ‡");
    println!("   - è®¡æ•°å™¨å’Œä»ªè¡¨å€¼è®¾ç½®æˆåŠŸ");
}

#[tokio::test]
async fn test_monitoring_performance_under_load() {
    println!("ğŸ§ª æµ‹è¯•ç›‘æ§ç³»ç»Ÿè´Ÿè½½æ€§èƒ½");
    
    let config = create_test_config();
    let mut manager = MonitoringManager::new(config);
    
    let start_time = std::time::Instant::now();
    let operations = 10000;
    
    // æ¨¡æ‹Ÿé«˜è´Ÿè½½æ“ä½œ
    for i in 0..operations {
        manager.increment_counter("load_test_operations", 1);
        
        if i % 100 == 0 {
            let metric = MetricPoint {
                name: "load_test_latency".to_string(),
                metric_type: MetricType::Histogram,
                value: (i as f64 % 10.0) + 1.0,
                labels: HashMap::new(),
                timestamp: Utc::now(),
                help: None,
            };
            manager.record_metric(metric);
        }
    }
    
    let duration = start_time.elapsed();
    let ops_per_sec = operations as f64 / duration.as_secs_f64();
    
    // éªŒè¯æ€§èƒ½è¦æ±‚
    assert!(ops_per_sec > 1000.0, "ç›‘æ§ç³»ç»Ÿåº”è¯¥æ”¯æŒ >1000 ops/secï¼Œå®é™…: {:.0}", ops_per_sec);
    
    println!("âœ… ç›‘æ§ç³»ç»Ÿè´Ÿè½½æ€§èƒ½æµ‹è¯•é€šè¿‡");
    println!("   - å¤„ç†æ“ä½œæ•°: {}", operations);
    println!("   - æ€»è€—æ—¶: {:.2}ms", duration.as_millis());
    println!("   - æ€§èƒ½: {:.0} ops/sec", ops_per_sec);
}

#[tokio::test]
async fn test_benchmark_accuracy() {
    println!("ğŸ§ª æµ‹è¯•åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§");
    
    let config = create_test_config();
    let mut manager = MonitoringManager::new(config);
    
    // è¿è¡Œå¤šæ¬¡å°è§„æ¨¡åŸºå‡†æµ‹è¯•
    let mut results = Vec::new();
    for i in 0..3 {
        let result = manager.run_performance_benchmark(&format!("accuracy_test_{}", i), 100);
        results.push(result);
    }
    
    // éªŒè¯ç»“æœä¸€è‡´æ€§
    for (i, result) in results.iter().enumerate() {
        assert_eq!(result.iterations, 100);
        assert!(result.throughput_ops_per_sec > 0.0);
        println!("   æµ‹è¯• {}: {:.0} ops/sec, å¹³å‡å»¶è¿Ÿ: {:.3}ms", 
                i, result.throughput_ops_per_sec, result.avg_latency_ms);
    }
    
    println!("âœ… åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§éªŒè¯é€šè¿‡");
}
